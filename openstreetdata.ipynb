{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87cdc961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession,functions as F\n",
    "from pyspark.sql.functions import col, split,hour, when,udf,array_contains,dayofweek\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import ast\n",
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import shutil, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3ad15a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----------+--------+\n",
      "|             name|       lat|        lon|    type|\n",
      "+-----------------+----------+-----------+--------+\n",
      "|โรงพยาบาลมหาชัย 2|13.7072628|100.3009969|hospital|\n",
      "+-----------------+----------+-----------+--------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-----------+--------------+-----------+\n",
      "|       lat|        lon|          name|       type|\n",
      "+----------+-----------+--------------+-----------+\n",
      "|13.7477023|100.5345542|         Manna|      Other|\n",
      "|13.6512018|100.4876955|      7-Eleven|   7-Eleven|\n",
      "| 13.650673|100.4882352|      7-Eleven|   7-Eleven|\n",
      "|13.7185734|100.5673621|         Shell|gas_station|\n",
      "|13.7282777|100.5330447|      7-Eleven|   7-Eleven|\n",
      "|13.7281063|100.5331414|           Zen|      Other|\n",
      "|13.7272399|100.5335418|     Pola Pola|       cafe|\n",
      "|13.7284508|100.5358805|      7-Eleven|   7-Eleven|\n",
      "|13.7289158|100.5355833|  Cafe Bangrak|       cafe|\n",
      "|13.7288589|100.5356641|      7-Eleven|   7-Eleven|\n",
      "|13.7272956|100.5364037|        Mimy's|       cafe|\n",
      "|13.7268964|100.5367763|       Zanotti|      Other|\n",
      "|13.7269174|100.5370033|    Cafe Turin|       cafe|\n",
      "|13.7269308|100.5416167|      7-Eleven|   7-Eleven|\n",
      "|13.7269776|100.5416951|The Front Page|      Other|\n",
      "|13.7269994|100.5441236|           N/A|    toilets|\n",
      "|13.7285979| 100.542408|           N/A|    toilets|\n",
      "|13.7253756| 100.537783|    Cafe D'Oro|       cafe|\n",
      "|14.1066971|100.6190183|      7-Eleven|   7-Eleven|\n",
      "|14.1013869|100.6189359|        Caltex|gas_station|\n",
      "+----------+-----------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "spark = SparkSession.builder.appName(\"711_busstop\").getOrCreate()\n",
    "\n",
    "query = \"\"\"\n",
    "[out:json][timeout:100];\n",
    "(\n",
    "    node[\"shop\"=\"convenience\"][\"name\"=\"7-Eleven\"](13,100.3,14.5,101.5);\n",
    "    node[\"amenity\"=\"bus_station\"](13,100.3,14.5,101.5);\n",
    "    node[\"amenity\"=\"place_of_worship\"](13,100.3,14.5,101.5);\n",
    "    node[\"amenity\"=\"police\"](13,100.3,14.5,101.5);\n",
    "    node[\"amenity\"=\"waste_disposal\"](13,100.3,14.5,101.5);\n",
    "    node[\"amenity\"=\"marketplace\"](13,100.3,14.5,101.5);\n",
    "    node[\"amenity\"=\"toilets\"](13,100.3,14.5,101.5);\n",
    "    node[\"amenity\"=\"cafe\"](13,100.3,14.5,101.5);\n",
    "    node[\"amenity\"=\"restaurant\"](13,100.3,14.5,101.5);\n",
    "    node[\"amenity\"=\"fuel\"](13,100.3,14.5,101.5);\n",
    "    node[\"amenity\"=\"taxi\"](13,100.3,14.5,101.5);\n",
    "    node[\"amenity\"=\"hospital\"](13,100.3,14.5,101.5);\n",
    ");\n",
    "out body;\n",
    "\"\"\"\n",
    "url = 'https://overpass-api.de/api/interpreter'\n",
    "res = requests.get(url, params={'data': query})\n",
    "\n",
    "kind_mapping = {\n",
    "    'shop': {'convenience': '7-Eleven'},\n",
    "    'amenity': {\n",
    "        'bus_station': 'bus_station',\n",
    "        'place_of_worship': 'PlaceOfWorship',\n",
    "        'police': 'PoliceStation',\n",
    "        'marketplace': 'marketplace',\n",
    "        'toilets': 'toilets',\n",
    "        'cafe': 'cafe',\n",
    "        'fuel': 'gas_station',\n",
    "        'taxi': 'taxi_terminal',\n",
    "        'hospital': 'hospital',\n",
    "        'waste_disposal': 'waste_disposal'\n",
    "    }\n",
    "}\n",
    "\n",
    "if res.status_code == 200:\n",
    "    data = res.json()\n",
    "    if 'elements' in data:\n",
    "        records = []\n",
    "        for elem in data['elements']:\n",
    "            tags = elem.get('tags', {})\n",
    "            name = tags.get('name', 'N/A')\n",
    "            lat = elem['lat']\n",
    "            lon = elem['lon']\n",
    "\n",
    "            kind = 'Other'\n",
    "            \n",
    "            if tags.get('shop') == 'convenience' and name == '7-Eleven':\n",
    "                kind = '7-Eleven'\n",
    "            elif 'amenity' in tags:\n",
    "                kind = kind_mapping['amenity'].get(tags['amenity'], kind)\n",
    "    \n",
    "            records.append({'name': name, 'lat': lat, 'lon': lon, 'type': kind})\n",
    "\n",
    "        # To Spark DataFrame\n",
    "        df = spark.createDataFrame(records)\n",
    "        df.orderBy(\"lon\").select(\"name\", \"lat\", \"lon\", \"type\").show(1)\n",
    "        df.show()\n",
    "\n",
    "        # Save single CSV\n",
    "        df.coalesce(1).write.csv('combined', header=True, mode='overwrite')\n",
    "        part_file = glob.glob('combined/part-*.csv')[0]\n",
    "        shutil.move(part_file, 'location.csv')\n",
    "        shutil.rmtree('combined')\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "else:\n",
    "    print(f\"Error: {res.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36615f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark data transformation\n",
    "spark = SparkSession.builder.appName(\"fondue\").getOrCreate()\n",
    "df = spark.read.csv(\n",
    "    \"bangkok_traffy.csv\",  # Replace with your actual file path\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    multiLine=True,\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    ignoreLeadingWhiteSpace=True,\n",
    "    ignoreTrailingWhiteSpace=True,\n",
    "    timestampFormat=\"yyyy-MM-dd HH:mm:ss.SSSSSS+00\"\n",
    ")\n",
    "sample_data = df.head(3)\n",
    "sampledf = spark.createDataFrame(sample_data, df.schema)\n",
    "\n",
    "\n",
    "def parse_set(value):\n",
    "    if value is not None:\n",
    "        return value.strip('{}').split(',')\n",
    "    else:\n",
    "        return [None]\n",
    "parse_set_udf = udf(parse_set, ArrayType(StringType()))\n",
    "df = df.withColumn(\"type\", parse_set_udf(col(\"type\")))\n",
    "\n",
    "type_list = ['ห้องน้ำ','คนจรจัด','การเดินทาง','จราจร','สอบถาม','ป้ายจราจร','ทางเท้า','ท่อระบายน้ำ','ถนน','กีดขวาง','ความสะอาด',\n",
    "             'สะพาน','ต้นไม้','ร้องเรียน','เสนอแนะ','เสียงรบกวน','สัตว์จรจัด','ความปลอดภัย','สายไฟ','แสงสว่าง','คลอง','น้ำท่วม','PM2.5','ป้าย']\n",
    "for category in type_list:\n",
    "    df = df.withColumn(category, when(array_contains(col(\"type\"),category), 1).otherwise(0))\n",
    "df = df.drop('type')\n",
    "\n",
    "#change เสร็จสิ้น to 0,1\n",
    "df = df.withColumn(\n",
    "    \"state\",\n",
    "    F.when(F.col(\"state\") == \"เสร็จสิ้น\", 1)\n",
    "    .when(F.col(\"state\") == \"กำลังดำเนินการ\", 0)\n",
    "      # keep the original value for other states\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#timestamp\n",
    "#df = df.withColumn(\"type\", explode(df[\"type\"]))\n",
    "df = df.withColumn(\n",
    "    \"timestamp_category\",\n",
    "    when((hour(col(\"timestamp\")) >= 5) & (hour(col(\"timestamp\")) < 12), \"Morning\")\n",
    "    .when((hour(col(\"timestamp\")) >= 12) & (hour(col(\"timestamp\")) < 17), \"Afternoon\")\n",
    "    .when((hour(col(\"timestamp\")) >= 17) & (hour(col(\"timestamp\")) < 20), \"Evening\")\n",
    "    .when((hour(col(\"timestamp\")) >= 20) & (hour(col(\"timestamp\")) < 24), \"Night\")\n",
    "    .otherwise(\"Late Night\")  #noted that it will gmt+7 as thai timezone\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"last_activity_category\",\n",
    "    when((hour(col(\"last_activity\")) >= 5) & (hour(col(\"last_activity\")) < 12), \"Morning\")\n",
    "    .when((hour(col(\"last_activity\")) >= 12) & (hour(col(\"last_activity\")) < 17), \"Afternoon\")\n",
    "    .when((hour(col(\"last_activity\")) >= 17) & (hour(col(\"last_activity\")) < 20), \"Evening\")\n",
    "    .when((hour(col(\"last_activity\")) >= 20) & (hour(col(\"last_activity\")) < 24), \"Night\")\n",
    "    .otherwise(\"Late Night\")  #noted that it will gmt+7 as thai timezone\n",
    ")\n",
    "df = df.withColumn(\"timestamp_is_weekend\", dayofweek(col(\"timestamp\")).isin([1, 7]))\n",
    "df = df.withColumn(\"last_activity_is_weekend\", dayofweek(col(\"last_activity\")).isin([1, 7]))\n",
    "\n",
    "\n",
    "df.select(\"state\").show(3)\n",
    "df.filter(df.timestamp_is_weekend == True).limit(1).show()\n",
    "df.groupBy('organization').count().orderBy(F.desc('count')).show(10)\n",
    "df.printSchema()\n",
    "print(df.count())\n",
    "#how can we detect photo before and after\n",
    "\n",
    "#df.groupBy(\"district\").count().orderBy(F.desc(\"count\")).show(5)\n",
    "#df.groupBy(\"district\").count().orderBy(F.asc(\"count\")).show(5)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
